# Configuration for Rainbow DQN Trading Agent Training

agent:
  # Core RL params
  gamma: 0.99
  lr: 0.0001 # Learning rate
  batch_size: 256
  replay_buffer_size: 500000
  target_update_freq: 500 # Steps to update target network

  # Network architecture
  window_size: 60
  n_features: 5
  hidden_dim: 128
  num_actions: 7 # Needs to match environment action space

  # Transformer specific params (add if missing)
  nhead: 4 # Number of attention heads
  num_encoder_layers: 2 # Example: Number of Transformer encoder layers
  dim_feedforward: 256 # Example: Feedforward dimension
  transformer_dropout: 0.1 # Example: Dropout within transformer

  # Rainbow specific params
  n_steps: 3          # Multi-step returns
  num_atoms: 51       # Distributional RL atoms
  v_min: -11.0        # Distributional RL support min value (Adjusted for new reward scale)
  v_max: 11.0         # Distributional RL support max value (Adjusted for new reward scale)
  alpha: 0.6          # PER priority exponent
  beta_start: 0.4     # PER importance sampling exponent (initial)
  beta_frames: 100000 # PER beta annealing frames (steps)

  # Other agent params
  grad_clip_norm: 10.0 # Gradient clipping norm
  debug: false        # Enable debug checks (e.g., gradient checks)

environment:
  initial_balance: 10000.0
  transaction_fee: 0.001 # Percentage fee
  reward_pnl_scale: 500.0 # Scale factor for PnL component of reward (if used)
  reward_cost_scale: 200.0 # Scale factor for transaction cost penalty in reward

trainer:
  seed: 42
  warmup_steps: 25000     # Steps with random actions before training
  update_freq: 4          # Agent learning update frequency (steps)
  log_freq: 60            # Logging frequency (steps)
  validation_freq: 50     # Validation frequency (episodes)
  checkpoint_save_freq: 5 # Checkpoint save frequency (episodes)
  reward_window: 10       # Window for averaging episode rewards in logs
  early_stopping_patience: 5 # Episodes without validation improvement to stop
  min_validation_threshold: 0.0 # Minimum score change considered improvement

# --- Run Configuration --- #
run:
  mode: 'train'              # 'train' or 'eval'
  episodes: 1000            # Number of training episodes
  model_dir: 'models'        # Directory to save models and logs
  resume: false             # Resume training from latest checkpoint if true
  specific_file: null       # Path to a specific training file (relative to data root) or null/None
  skip_evaluation: false    # Skip final evaluation on test set after training
  # data_base_dir: 'data'   # Optional: Override base data directory (defaults to 'data')
  # eval_model_prefix: 'models/rainbow_transformer_best' # Optional: Model prefix for eval mode 